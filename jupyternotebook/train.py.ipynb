{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multi_input_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21992/1446204794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmulti_input_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiMLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'multi_input_model'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import threading\n",
    "import numpy as np\n",
    "from multi_input_model import MultiMLP\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as sta\n",
    "from random import shuffle\n",
    "from utils import read_data, checkpoint\n",
    "from data_util_combined_model import BothDataset\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----Start Training----\n"
     ]
    }
   ],
   "source": [
    "START = time.time()\n",
    "print(\" ----Start Training----\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Declare some lists for train data\n",
    "LMGround_truth, KFGround_truth,   = [], []\n",
    "LMeta_in, LMlam_in, KFeta_in, KFlam_in = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare some lists for valid data\n",
    "VLMeta_in, VLMlam_in, VKFeta_in, VKFlam_in = [], [], [], []\n",
    "VKFGround_truth, VLMGround_truth = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading LM Data\n",
    "train_num = 1\n",
    "\n",
    "LM_Path = '../data/LM_s_data/'\n",
    "LM_data_list = glob.glob(LM_Path+'*.txt')\n",
    "#shuffle(LM_data_list)\n",
    "LM_train_data_list = LM_data_list[ : train_num]\n",
    "LM_valid_data_list = LM_data_list[ :  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading KF Data\n",
    "KF_Path = '../data/KF_s_data/'\n",
    "KF_data_list = glob.glob(KF_Path+'*.txt')\n",
    "#shuffle(KF_data_list)\n",
    "KF_train_data_list = KF_data_list[ : train_num]\n",
    "KF_valid_data_list = KF_data_list[ : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading LM valid data\n",
    "def load_lm_valid_data():\n",
    "\n",
    "    for valid_data in LM_valid_data_list :\n",
    "        m = read_data.read_data(valid_data)\n",
    "        for i in range (41) :\n",
    "            for j in range (len(m[i])):\n",
    "                Vfactor_eta = m[i][j][0][:9]\n",
    "                Vfactor_lam = m[i][j][0][9:]\n",
    "                Vmessage_eta = m[i][j][1][:3]\n",
    "                Vmessage_lam = m[i][j][1][3:]\n",
    "\n",
    "                tmp = [m[i + k][j][2] for k in range(10)]\n",
    "                VLMGround_truth.append(np.sum(tmp, axis = 0))\n",
    "                \n",
    "                VLMeta_in.append(Vfactor_eta + Vmessage_eta)\n",
    "                VLMlam_in.append(Vfactor_lam + Vmessage_lam)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading KF valid data\n",
    "def load_kf_valid_data():\n",
    "\n",
    "    for valid_data in KF_valid_data_list :\n",
    "        m = read_data.read_data(valid_data)\n",
    "        for i in range (41) :\n",
    "            for j in range (len(m[i])):\n",
    "                Vfactor_eta = m[i][j][0][:9]\n",
    "                Vfactor_lam = m[i][j][0][9:]\n",
    "                Vmessage_eta = m[i][j][1][:3]\n",
    "                Vmessage_lam = m[i][j][1][3:]\n",
    "                \n",
    "                tmp = [m[i + k][j][2] for k in range(10)]\n",
    "                VKFGround_truth.append(np.sum(tmp, axis = 0))\n",
    "                \n",
    "                VKFeta_in.append(Vfactor_eta + Vmessage_eta)\n",
    "                VKFlam_in.append(Vfactor_lam + Vmessage_lam)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set the threading\n",
    "load_lm_valid_thread = threading.Thread(target = load_lm_valid_data)\n",
    "load_kf_valid_thread = threading.Thread(target = load_kf_valid_data)\n",
    "load_lm_valid_thread.start()\n",
    "load_kf_valid_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading LM training data\n",
    "for LM_train_data in LM_train_data_list :\n",
    "    m = read_data.read_data(LM_train_data)\n",
    "    for i in range (41) :\n",
    "        for j in range (len(m[i])):\n",
    "                factor_eta = m[i][j][0][:9]\n",
    "                factor_lam = m[i][j][0][9:]\n",
    "                message_eta = m[i][j][1][:3]\n",
    "                message_lam = m[i][j][1][3:]\n",
    "                \n",
    "                tmp = [m[i + k][j][2] for k in range(10)]\n",
    "                LMGround_truth.append(np.sum(tmp, axis = 0))\n",
    "                \n",
    "                LMeta_in.append(factor_eta + message_eta)\n",
    "                LMlam_in.append(factor_lam + message_lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading KF training data\n",
    "for KF_train_data in KF_train_data_list :\n",
    "    m = read_data.read_data(KF_train_data)\n",
    "    for i in range (41) :\n",
    "        for j in range (len(m[i])):\n",
    "                factor_eta = m[i][j][0][:9]\n",
    "                factor_lam = m[i][j][0][9:]\n",
    "                message_eta = m[i][j][1][:3]\n",
    "                message_lam = m[i][j][1][3:]\n",
    "                \n",
    "                tmp = [m[i + k][j][2] for k in range(10)]\n",
    "                KFGround_truth.append(np.sum(tmp, axis = 0))\n",
    "\n",
    "                KFeta_in.append(factor_eta + message_eta)\n",
    "                KFlam_in.append(factor_lam + message_lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadining data in  3.9483375549316406\n",
      "13858 13858 13858 13858 13858 13858\n",
      "13858 13858 13858 13858 13858 13858\n"
     ]
    }
   ],
   "source": [
    "load_lm_valid_thread.join()\n",
    "load_kf_valid_thread.join()\n",
    "print('Loadining data in ',time.time() - START)\n",
    "print(len(LMGround_truth), len(KFGround_truth), len(LMeta_in), len(LMlam_in), len(KFeta_in), len(KFlam_in) )\n",
    "print(len(VLMGround_truth), len(VKFGround_truth), len(VLMeta_in), len(VLMlam_in), len(VKFeta_in), len(VKFlam_in) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with 13858 data\n",
      "Valid with 13858 data\n"
     ]
    }
   ],
   "source": [
    "training_data = BothDataset(LMeta_in, LMlam_in, LMGround_truth, KFeta_in, KFlam_in, KFGround_truth)\n",
    "validation_data = BothDataset(VLMeta_in, VLMlam_in, VLMGround_truth, VKFeta_in, VKFlam_in, VKFGround_truth)\n",
    "print(f'Train with {training_data.n_samples} data')\n",
    "print(f'Valid with {validation_data.n_samples} data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting training parameters\n",
    "Epochs = 300\n",
    "Batch_size = 64\n",
    "lr = 1e-3\n",
    "Best_valid_loss = 10000\n",
    "\n",
    "train_loader = DataLoader(dataset = training_data, batch_size = Batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(dataset = validation_data, batch_size = 1, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting Networks\n",
    "model = MultiMLP(512, 512, 512)\n",
    "\n",
    "# Initial model's weight\n",
    "model.initialize()\n",
    "model.to(device)\n",
    "\n",
    "# Setting training function\n",
    "Optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "Scheduler = torch.optim.lr_scheduler.StepLR(Optimizer, step_size = 1, gamma = 0.99)\n",
    "criterion = torch.nn.HuberLoss(reduction ='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    n_items = len(y)\n",
    "    pred = y_pred.view(n_items,-1)\n",
    "    y = y.view(n_items, -1)\n",
    "    n_correct = torch.sum((torch.abs(pred - y) < torch.abs(0.01 * y)))\n",
    "    acc = (n_correct.item() * 100.0 / n_items)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, Optimizer, Scheduler, train_loader) :\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for iter, train_data in enumerate(train_loader, 0):\n",
    "\n",
    "            LMeta_in, LMlam_in, LMGround_truth, KFeta_in, KFlam_in, KFGround_truth = train_data\n",
    "\n",
    "            Optimizer.zero_grad()\n",
    "\n",
    "            etal, laml, etak, lamk = model(LMeta_in, LMlam_in, KFeta_in, KFlam_in)\n",
    "\n",
    "            LMoutputs = torch.cat((etal, laml), -1)\n",
    "            KFoutputs = torch.cat((etak, lamk), -1)\n",
    "            LMloss = criterion(LMoutputs, LMGround_truth)\n",
    "            KFloss = criterion(KFoutputs, KFGround_truth)\n",
    "            LMacc = calculate_accuracy(LMoutputs, LMGround_truth)\n",
    "            KFacc = calculate_accuracy(KFoutputs, KFGround_truth)\n",
    "\n",
    "            loss = (LMloss + KFloss) / 2\n",
    "            acc = (LMacc + KFacc) / 2\n",
    "            loss.backward()\n",
    "\n",
    "            Optimizer.step()\n",
    "\n",
    "            Scheduler.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_acc += acc\n",
    "\n",
    "#             for parameters in model.parameters():\n",
    "#                 print(parameters)\n",
    "    return epoch_train_loss / len(train_loader), epoch_train_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, criterion, valid_loader) :\n",
    "\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad() :\n",
    "\n",
    "        for iter, valid_data in enumerate(valid_loader, 0):\n",
    "\n",
    "                VLMeta_in, VLMlam_in, VLMGround_truth, VKFeta_in, VKFlam_in, VKFGround_truth = valid_data\n",
    "\n",
    "                etal, laml, etak, lamk = model(VLMeta_in, VLMlam_in, VKFeta_in, VKFlam_in)\n",
    "\n",
    "                VLMoutputs = torch.cat((etal, laml), -1)\n",
    "                VKFoutputs = torch.cat((etak, lamk), -1)\n",
    "                \n",
    "                VLMloss = criterion(VLMoutputs, VLMGround_truth)\n",
    "                VKFloss = criterion(VKFoutputs, VKFGround_truth)\n",
    "                loss = (VLMloss + VKFloss) / 2\n",
    "\n",
    "                VLMacc = calculate_accuracy(VLMoutputs, VLMGround_truth)\n",
    "                VKFacc = calculate_accuracy(VKFoutputs, VKFGround_truth)\n",
    "                acc = (VLMacc + VKFacc) / 2\n",
    "\n",
    "                epoch_valid_loss += loss.item()\n",
    "                epoch_valid_acc += acc\n",
    "\n",
    "        global Best_valid_loss\n",
    "        if epoch_valid_loss < Best_valid_loss :\n",
    "            Best_valid_loss = epoch_valid_loss\n",
    "            model_state_dict = model.state_dict()\n",
    "            torch.save(model_state_dict, '../models/combine_model.pkl')\n",
    "            #checkpoint.save_checkpoint_state('combine', epoch, Optimizer, Scheduler)\n",
    "\n",
    "    return epoch_valid_loss / len(valid_loader), epoch_valid_acc / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/300// One epoch in 28.999900341033936\n",
      "--------------------\n",
      "Train loss : 0.439926// Train acc : 9.815711%\n",
      "Valid loss : 0.333557 // Valid acc : 12.833742%\n",
      "\n",
      "Epoch2/300// One epoch in 28.63015866279602\n",
      "--------------------\n",
      "Train loss : 0.290931// Train acc : 14.823843%\n",
      "Valid loss : 0.264690 // Valid acc : 16.091788%\n",
      "\n",
      "Epoch3/300// One epoch in 28.938877820968628\n",
      "--------------------\n",
      "Train loss : 0.258976// Train acc : 16.726882%\n",
      "Valid loss : 0.255284 // Valid acc : 17.116467%\n",
      "\n",
      "Epoch4/300// One epoch in 28.799429178237915\n",
      "--------------------\n",
      "Train loss : 0.254619// Train acc : 16.827901%\n",
      "Valid loss : 0.254309 // Valid acc : 16.856689%\n",
      "\n",
      "Epoch5/300// One epoch in 28.71240210533142\n",
      "--------------------\n",
      "Train loss : 0.254066// Train acc : 16.960474%\n",
      "Valid loss : 0.254204 // Valid acc : 16.957714%\n",
      "\n",
      "Epoch6/300// One epoch in 28.690035581588745\n",
      "--------------------\n",
      "Train loss : 0.254509// Train acc : 16.944802%\n",
      "Valid loss : 0.254193 // Valid acc : 16.961322%\n",
      "\n",
      "Epoch7/300// One epoch in 28.882170915603638\n",
      "--------------------\n",
      "Train loss : 0.253994// Train acc : 16.972757%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch8/300// One epoch in 29.668243885040283\n",
      "--------------------\n",
      "Train loss : 0.254205// Train acc : 16.966403%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch9/300// One epoch in 27.874449968338013\n",
      "--------------------\n",
      "Train loss : 0.254141// Train acc : 16.963227%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch10/300// One epoch in 27.939019918441772\n",
      "--------------------\n",
      "Train loss : 0.254434// Train acc : 16.975934%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch11/300// One epoch in 27.856990337371826\n",
      "--------------------\n",
      "Train loss : 0.254068// Train acc : 16.956873%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch12/300// One epoch in 27.765949249267578\n",
      "--------------------\n",
      "Train loss : 0.254407// Train acc : 16.956873%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch13/300// One epoch in 28.643874883651733\n",
      "--------------------\n",
      "Train loss : 0.253998// Train acc : 16.953697%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch14/300// One epoch in 29.444984197616577\n",
      "--------------------\n",
      "Train loss : 0.254351// Train acc : 16.966403%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch15/300// One epoch in 31.063433408737183\n",
      "--------------------\n",
      "Train loss : 0.254514// Train acc : 16.966403%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch16/300// One epoch in 31.224175691604614\n",
      "--------------------\n",
      "Train loss : 0.254255// Train acc : 16.966403%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch17/300// One epoch in 30.639883041381836\n",
      "--------------------\n",
      "Train loss : 0.254214// Train acc : 16.950520%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch18/300// One epoch in 34.66632795333862\n",
      "--------------------\n",
      "Train loss : 0.254223// Train acc : 16.975934%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch19/300// One epoch in 31.820474863052368\n",
      "--------------------\n",
      "Train loss : 0.253975// Train acc : 16.985464%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch20/300// One epoch in 29.648988008499146\n",
      "--------------------\n",
      "Train loss : 0.254221// Train acc : 16.979110%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch21/300// One epoch in 30.14990997314453\n",
      "--------------------\n",
      "Train loss : 0.254061// Train acc : 16.960050%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch22/300// One epoch in 33.05993366241455\n",
      "--------------------\n",
      "Train loss : 0.253978// Train acc : 16.972757%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch23/300// One epoch in 29.392070770263672\n",
      "--------------------\n",
      "Train loss : 0.253977// Train acc : 16.950520%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch24/300// One epoch in 28.015034198760986\n",
      "--------------------\n",
      "Train loss : 0.254149// Train acc : 16.956873%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch25/300// One epoch in 28.539023876190186\n",
      "--------------------\n",
      "Train loss : 0.254159// Train acc : 16.956873%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch26/300// One epoch in 28.582289934158325\n",
      "--------------------\n",
      "Train loss : 0.254234// Train acc : 16.953697%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch27/300// One epoch in 28.188452005386353\n",
      "--------------------\n",
      "Train loss : 0.254063// Train acc : 16.979110%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch28/300// One epoch in 29.768235206604004\n",
      "--------------------\n",
      "Train loss : 0.254066// Train acc : 16.944167%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch29/300// One epoch in 31.924790620803833\n",
      "--------------------\n",
      "Train loss : 0.254022// Train acc : 16.960050%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch30/300// One epoch in 28.587311267852783\n",
      "--------------------\n",
      "Train loss : 0.254009// Train acc : 16.972757%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch31/300// One epoch in 29.444194555282593\n",
      "--------------------\n",
      "Train loss : 0.254154// Train acc : 16.979110%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n",
      "Epoch32/300// One epoch in 28.12358784675598\n",
      "--------------------\n",
      "Train loss : 0.254052// Train acc : 16.956873%\n",
      "Valid loss : 0.254192 // Valid acc : 16.964930%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "open('./train.txt', 'w')\n",
    "for num_epoch in range(Epochs) :\n",
    "\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    Train_loss, Train_acc = train_one_epoch(model, criterion, Optimizer, Scheduler, train_loader)\n",
    "\n",
    "    Valid_loss, Valid_acc = validation(num_epoch + 1, model, criterion, valid_loader)\n",
    "\n",
    "\n",
    "    with open('./train.txt', 'a') as log:\n",
    "        log.writelines(f\"epoch : {num_epoch + 1}, \\\n",
    "                Train_loss :  {Train_loss:.6f}, \\\n",
    "                Train_acc : {Train_acc:.6f},\\\n",
    "                Valid_loss : {Valid_loss:.6f},\\\n",
    "                Valid_acc : {Valid_acc:.6f}\\n\")\n",
    "\n",
    "\n",
    "    print(f'Epoch{num_epoch + 1}/{Epochs}// One epoch in {time.time() - start_epoch}')\n",
    "    print('--------------------')\n",
    "    print(f'Train loss : {Train_loss:.6f}// Train acc : {Train_acc:.6f}%\\nValid loss : {Valid_loss:.6f} // Valid acc : {Valid_acc:.6f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
